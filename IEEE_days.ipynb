{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IEEE_days.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": [
        "0AG3GQhMKRvC"
      ]
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "xVngc4FVWich",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Run this first"
      ]
    },
    {
      "metadata": {
        "id": "Mj62Kmf87Npt",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 4
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "125dea78-b93c-4c5e-8a61-c8745104d203",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517753642623,
          "user_tz": -60,
          "elapsed": 1507,
          "user": {
            "displayName": "Javier Antoran",
            "photoUrl": "//lh3.googleusercontent.com/-33SpdZ5pNBY/AAAAAAAAAAI/AAAAAAAAM2k/UTfVAawQnOQ/s50-c-k-no/photo.jpg",
            "userId": "116655854548442559250"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import time  # This is required to include time module.\n",
        "import numpy as np\n",
        "import scipy.io as sio\n",
        "\n",
        "!wget http://www.ucode.es/data.mat\n",
        "!ls"
      ],
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-02-04 14:13:59--  http://www.ucode.es/data.mat\n",
            "Resolving www.ucode.es (www.ucode.es)... 151.101.53.147, 2a04:4e42:d::403\n",
            "Connecting to www.ucode.es (www.ucode.es)|151.101.53.147|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7468024 (7.1M) [application/octet-stream]\n",
            "Saving to: ‘data.mat.7’\n",
            "\n",
            "data.mat.7          100%[===================>]   7.12M  21.3MB/s    in 0.3s    \n",
            "\n",
            "2018-02-04 14:13:59 (21.3 MB/s) - ‘data.mat.7’ saved [7468024/7468024]\n",
            "\n",
            "datalab   data.mat.1  data.mat.3  data.mat.5  data.mat.7\n",
            "data.mat  data.mat.2  data.mat.4  data.mat.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0AG3GQhMKRvC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# This method loads the training, validation and test set.\n",
        "    # It also divides the training set into mini-batches.\n",
        "    # Inputs:\n",
        "    #   N: Mini-batch size.\n",
        "    # Outputs:\n",
        "    #   train_input: An array of size D X N X M, where\n",
        "    #                 D: number of input dimensions (in this case, 3).\n",
        "    #                 N: size of each mini-batch (in this case, 100).\n",
        "    #                 M: number of minibatches.\n",
        "    #   train_target: An array of size 1 X N X M.\n",
        "    #   valid_input: An array of size D X number of points in the validation set.\n",
        "    #   test: An array of size D X number of points in the test set.\n",
        "    #   vocab: Vocabulary containing index to word mapping."
      ]
    },
    {
      "metadata": {
        "id": "eWkkBCC3-KFq",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "da8c3ae6-8c6c-43fe-cce0-e46d7b9ad2c5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517753643730,
          "user_tz": -60,
          "elapsed": 454,
          "user": {
            "displayName": "Javier Antoran",
            "photoUrl": "//lh3.googleusercontent.com/-33SpdZ5pNBY/AAAAAAAAAAI/AAAAAAAAM2k/UTfVAawQnOQ/s50-c-k-no/photo.jpg",
            "userId": "116655854548442559250"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def load_data(batch_size=100):\n",
        "  \n",
        "\n",
        "  data = sio.loadmat('data.mat')\n",
        "  #data\n",
        "  test_data = data['data'][0][0][0]\n",
        "  train_data = data['data'][0][0][1]\n",
        "  valid_data = data['data'][0][0][2]\n",
        "  vocab = data['data'][0][0][3]\n",
        "  print(test_data.shape)\n",
        "  print(train_data.shape)\n",
        "  print(valid_data.shape)\n",
        "  print(vocab.shape)\n",
        "  \n",
        "  numdims = train_data.shape[0]\n",
        "  D = numdims - 1\n",
        "  M = train_data.shape[1] / batch_size\n",
        "  \n",
        "  \n",
        "  train_input = np.reshape(train_data[0:D, 0:batch_size * M], (D, batch_size, M))\n",
        "  train_target = np.reshape(train_data[D, 0:batch_size * M], (1, batch_size, M))\n",
        "  valid_input = valid_data[0:D, :]\n",
        "  valid_target = valid_data[D, :]\n",
        "  test_input = test_data[0:D, :]\n",
        "  test_target = test_data[D, :]\n",
        "  vocab = vocab\n",
        "\n",
        "  print(train_input.shape)\n",
        "  print(train_target.shape)\n",
        "  print(valid_input.shape)\n",
        "  print(valid_target.shape)\n",
        "  print(test_input.shape)\n",
        "  print(test_target.shape)\n",
        "  print(vocab.shape)\n",
        "\n",
        "  return train_input, train_target, valid_input, valid_target, test_input, test_target, vocab\n"
      ],
      "execution_count": 312,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(4, 46568)\n",
            "(4, 372550)\n",
            "(4, 46568)\n",
            "(1, 250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TZknXuaW8Koz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Inputs:\n",
        "    #   epochs: Number of epochs to run.\n",
        "# Output:\n",
        "    #   model: A struct containing the learned weights and biases and vocabulary."
      ]
    },
    {
      "metadata": {
        "id": "iGfLlw6g8m1_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# SET HYPERPARAMETERS HERE."
      ]
    },
    {
      "metadata": {
        "id": "pxpCJxoJ8Bq0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "\n",
        "batchsize = 100 # Mini-batch size.\n",
        "learning_rate = 0.1 # Learning rate; default = 0.1.\n",
        "momentum = 0.5 # Momentum; default = 0.9.\n",
        "numhid1 = 50 # Dimensionality of embedding space; default = 50.\n",
        "numhid2 = 200 # Number of units in hidden layer; default = 200.\n",
        "init_wt = 0.01 # Standard deviation of the normal distribution\n",
        "# which is sampled to get the initial weights; default = 0.01\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3RoxtLRf8wpS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# VARIABLES FOR TRACKING TRAINING PROGRESS."
      ]
    },
    {
      "metadata": {
        "id": "OXmqvIAM8zu2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "show_training_CE_after = 100 # Frequency with which to show training cross-entropy error\n",
        "show_validation_CE_after = 1000 # Frequency with which to show validation cross-entropy error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LVreg7sV_tot",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LOAD DATA."
      ]
    },
    {
      "metadata": {
        "id": "TEUbuAkW82DI",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7ff954d8-784d-4f55-ed3a-de8fe7d3f444",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1517753646638,
          "user_tz": -60,
          "elapsed": 496,
          "user": {
            "displayName": "Javier Antoran",
            "photoUrl": "//lh3.googleusercontent.com/-33SpdZ5pNBY/AAAAAAAAAAI/AAAAAAAAM2k/UTfVAawQnOQ/s50-c-k-no/photo.jpg",
            "userId": "116655854548442559250"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "train_input, train_target, valid_input, valid_target, test_input, test_target, vocab = load_data(batchsize)\n",
        "numwords, batchsize, numbatches = train_input.shape\n",
        "vocab_size = vocab.shape[1]\n",
        "print(vocab_size)"
      ],
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "250\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tQjz0RAo_jbG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# INITIALIZE WEIGHTS AND BIASES."
      ]
    },
    {
      "metadata": {
        "id": "xS5JmtgI_kXm",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "word_embedding_weights =  np.random.normal(loc=0, scale=init_wt, size=(vocab_size, numhid1))\n",
        "embed_to_hid_weights = np.random.normal(loc=0, scale=init_wt, size=(numwords * numhid1, numhid2))\n",
        "hid_to_output_weights = np.random.normal(loc=0, scale=init_wt, size=(numhid2, vocab_size))\n",
        "hid_bias = np.zeros((numhid2, 1))\n",
        "output_bias = np.zeros((vocab_size, 1))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jWMi4HGE_ppL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "word_embedding_weights_delta = np.zeros((vocab_size, numhid1))\n",
        "word_embedding_weights_gradient = np.zeros((vocab_size, numhid1))\n",
        "embed_to_hid_weights_delta = np.zeros((numwords * numhid1, numhid2))\n",
        "hid_to_output_weights_delta = np.zeros((numhid2, vocab_size))\n",
        "hid_bias_delta = np.zeros((numhid2, 1))\n",
        "output_bias_delta = np.zeros((vocab_size, 1))\n",
        "expansion_matrix = np.eye(vocab_size)\n",
        "count = 0\n",
        "tiny = np.exp(-30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "t4yiiI9GJkfs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# This method forward propagates through a neural network.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*     Inputs:\n",
        " *       input_batch: The input data as a matrix of size numwords X batchsize where,\n",
        "         numwords is the number of words, batchsize is the number of data points.\n",
        "         So, if input_batch(i, j) = k then the ith word in data point j is word\n",
        "         index k of the vocabulary.\n",
        "    \n",
        " *      word_embedding_weights: Word embedding as a matrix of size\n",
        "         vocab_size X numhid1, where vocab_size is the size of the vocabulary\n",
        "         numhid1 is the dimensionality of the embedding space.\n",
        "    \n",
        " *     embed_to_hid_weights: Weights between the word embedding layer and hidden\n",
        "         layer as a matrix of soze numhid1*numwords X numhid2, numhid2 is the\n",
        "         number of hidden units.\n",
        "    \n",
        " *       hid_to_output_weights: Weights between the hidden layer and output softmax\n",
        "                   unit as a matrix of size numhid2 X vocab_size\n",
        "    \n",
        " *       hid_bias: Bias of the hidden layer as a matrix of size numhid2 X 1.\n",
        "    \n",
        " *       output_bias: Bias of the output layer as a matrix of size vocab_size X 1.\n",
        "    \n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "*     Outputs:\n",
        " *       embedding_layer_state: State of units in the embedding layer as a matrix of\n",
        "         size numhid1*numwords X batchsize\n",
        "    \n",
        " *       hidden_layer_state: State of units in the hidden layer as a matrix of size\n",
        "         numhid2 X batchsize\n",
        "    \n",
        " *      output_layer_state: State of units in the output layer as a matrix of size\n",
        "         vocab_size X batchsize\n",
        "    "
      ]
    },
    {
      "metadata": {
        "id": "EITZ4EiJJisY",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "def fprop(input_batch=None, word_embedding_weights=None,\n",
        "          embed_to_hid_weights=None, hid_to_output_weights=None,\n",
        "          hid_bias=None, output_bias=None):\n",
        "    \n",
        "\n",
        "    numwords, batchsize = input_batch.shape\n",
        "    vocab_size, numhid1 = word_embedding_weights.shape\n",
        "    numhid2 = embed_to_hid_weights.shape[1]\n",
        "\n",
        "    #% COMPUTE STATE OF WORD EMBEDDING LAYER.\n",
        "    # Look up the inputs word indices in the word_embedding_weights matrix.\n",
        "    foo = word_embedding_weights[np.ndarray.flatten(input_batch-1), :].T \n",
        "    \n",
        "    embedding_layer_state = np.reshape(foo, (numhid1 * numwords, -1))\n",
        "\n",
        "    #% COMPUTE STATE OF HIDDEN LAYER.\n",
        "    # Compute inputs to hidden units.\n",
        "    \n",
        "    inputs_to_hidden_units = np.matmul(embed_to_hid_weights.T,embedding_layer_state) + np.tile(hid_bias, (1, batchsize))\n",
        "\n",
        "    # Apply logistic activation function.\n",
        "    hidden_layer_state = np.divide(1,  (1 + np.exp(-inputs_to_hidden_units)))\n",
        "\n",
        "    #% COMPUTE STATE OF OUTPUT LAYER.\n",
        "    # Compute inputs to softmax.\n",
        "    inputs_to_softmax = np.matmul(hid_to_output_weights.T, hidden_layer_state) + np.tile(output_bias, (1, batchsize))\n",
        "    # Subtract maximum. \n",
        "    # Remember that adding or subtracting the same constant from each input to a\n",
        "    # softmax unit does not affect the outputs. Here we are subtracting maximum to\n",
        "    # make all inputs <= 0. This prevents overflows when computing their\n",
        "    # exponents.\n",
        "    inputs_to_softmax = inputs_to_softmax - np.tile(np.max(inputs_to_softmax), (vocab_size, 1))\n",
        "\n",
        "    # Compute exp.\n",
        "    output_layer_state = np.exp(inputs_to_softmax)\n",
        "\n",
        "    # Normalize to get probability distribution.\n",
        "    output_layer_state =  np.divide(output_layer_state, np.tile(np.sum(output_layer_state, 0), (vocab_size, 1)))\n",
        "    return embedding_layer_state, hidden_layer_state, output_layer_state"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xB3tZJ5rAiLu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Training Loop :)"
      ]
    },
    {
      "metadata": {
        "id": "Vex1IhD1ITNE",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            },
            {
              "item_id": 2
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "outputId": "dc9d6d9b-c6e6-4c8c-c81d-227ddba8d761",
        "executionInfo": {
          "status": "error",
          "timestamp": 1517756236691,
          "user_tz": -60,
          "elapsed": 582,
          "user": {
            "displayName": "Javier Antoran",
            "photoUrl": "//lh3.googleusercontent.com/-33SpdZ5pNBY/AAAAAAAAAAI/AAAAAAAAM2k/UTfVAawQnOQ/s50-c-k-no/photo.jpg",
            "userId": "116655854548442559250"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "tic_ = time.time()\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "  \n",
        "      print('Epoch %d\\n'% epoch)\n",
        "      this_chunk_CE = 0\n",
        "      trainset_CE = 0\n",
        "      \n",
        "       # LOOP OVER MINI-BATCHES.\n",
        "      for m in range(1, numbatches):\n",
        "          input_batch = train_input[:, :, m]\n",
        "          target_batch = train_target[:, :, m]\n",
        "          \n",
        "          # FORWARD PROPAGATE.\n",
        "          # Compute the state of each layer in the network given the input batch\n",
        "          # and all weights and biases\n",
        "          embedding_layer_state, hidden_layer_state, output_layer_state = \\\n",
        "          fprop(input_batch, word_embedding_weights, embed_to_hid_weights, hid_to_output_weights, hid_bias, output_bias)\n",
        "          \n",
        "           # COMPUTE DERIVATIVE.\n",
        "          #% Expand the target to a sparse 1-of-K vector.\n",
        "          expanded_target_batch = expansion_matrix[:, target_batch-1]\n",
        "          #% Compute derivative of cross-entropy loss function.\n",
        "          error_deriv = output_layer_state - np.squeeze(expanded_target_batch)\n",
        "          \n",
        "           # MEASURE LOSS FUNCTION.\n",
        "          CE = -np.sum(np.sum(np.multiply(expanded_target_batch, np.log(output_layer_state + tiny)))) / batchsize\n",
        "          count = count + 1\n",
        "          this_chunk_CE = this_chunk_CE + (CE - this_chunk_CE) / count\n",
        "          trainset_CE = trainset_CE + (CE - trainset_CE) / m\n",
        "          print 'Batch %d Train CE %1.3f'% (m, this_chunk_CE)\n",
        "          if m % show_training_CE_after == 0:\n",
        "              print('\\n')\n",
        "              count = 0\n",
        "              this_chunk_CE = 0\n",
        "\n",
        "          ################\n",
        "          # BACK PROPAGATE.\n",
        "          #% OUTPUT LAYER.\n",
        "          hid_to_output_weights_gradient = np.matmul(hidden_layer_state,  error_deriv.T)\n",
        "          output_bias_gradient = np.sum(error_deriv, 1)\n",
        "          back_propagated_deriv_1_ = np.multiply(np.matmul(hid_to_output_weights, error_deriv), hidden_layer_state)\n",
        "          back_propagated_deriv_1 = np.multiply(back_propagated_deriv_1_, (1 - hidden_layer_state))\n",
        "          \n",
        "\n",
        "          #% HIDDEN LAYER.\n",
        "\n",
        "          embed_to_hid_weights_gradient = np.matmul(embedding_layer_state, back_propagated_deriv_1.T)\n",
        "\n",
        "          hid_bias_gradient = np.sum(back_propagated_deriv_1, 1)\n",
        "\n",
        "          back_propagated_deriv_2 = np.matmul(embed_to_hid_weights, back_propagated_deriv_1)\n",
        "\n",
        "          word_embedding_weights_gradient[:] = 0\n",
        "          \n",
        "          \n",
        "          #% EMBEDDING LAYER.\n",
        "          for w in range(0, numwords):\n",
        "            bar = expansion_matrix[:, input_batch[w, :]-1]\n",
        "            bareto = back_propagated_deriv_2[(w * numhid1) : ((w+1) * numhid1), :].T\n",
        "            \n",
        "            foo = np.matmul(bar, bareto)\n",
        "            word_embedding_weights_gradient = word_embedding_weights_gradient + foo\n",
        "          \n",
        "          # UPDATE WEIGHTS AND BIASES.\n",
        "          word_embedding_weights_delta = np.multiply(momentum, word_embedding_weights_delta) + np.divide(word_embedding_weights_gradient, batchsize)\n",
        "          word_embedding_weights = word_embedding_weights - (learning_rate * word_embedding_weights_delta)\n",
        "\n",
        "          embed_to_hid_weights_delta = np.multiply(momentum, embed_to_hid_weights_delta) + (embed_to_hid_weights_gradient / batchsize)\n",
        "          embed_to_hid_weights = embed_to_hid_weights - (learning_rate * embed_to_hid_weights_delta)\n",
        "\n",
        "          hid_to_output_weights_delta = np.multiply(momentum, hid_to_output_weights_delta) + (hid_to_output_weights_gradient / batchsize)\n",
        "          hid_to_output_weights = hid_to_output_weights - (learning_rate * hid_to_output_weights_delta)\n",
        "\n",
        "          hid_bias_delta = (momentum * hid_bias_delta) + (np.reshape(hid_bias_gradient, (-1, 1)) / batchsize)\n",
        "          hid_bias = hid_bias - (learning_rate * hid_bias_delta)\n",
        "\n",
        "          output_bias_delta = np.multiply(momentum, output_bias_delta) + (np.reshape(output_bias_gradient, (-1, 1)) / batchsize)\n",
        "          output_bias = output_bias - (learning_rate * output_bias_delta)\n",
        "          \n",
        "          # VALIDATE.\n",
        "          if m % show_validation_CE_after == 0:\n",
        "              print('Running validation ...')\n",
        "              \n",
        "              embedding_layer_state, hidden_layer_state, output_layer_state = fprop(valid_input, word_embedding_weights, embed_to_hid_weights, hid_to_output_weights, hid_bias, output_bias)\n",
        "              datasetsize = valid_input.shape[1]\n",
        "              expanded_valid_target = expansion_matrix[:, valid_target-1]\n",
        "              CE = -np.sum(np.sum(np.multiply(expanded_valid_target, np.log(output_layer_state + tiny)))) / datasetsize\n",
        "              print 'Validation CE %1.3f \\n' % (CE)\n",
        "              \n",
        "          \n",
        "          \n",
        "print('Finished Training.\\n')\n",
        "\n",
        "  \n",
        "print'Final Training CE %.3f\\n' % trainset_CE\n",
        "\n",
        "toc_ = time.time()\n",
        "print('Training took %.2f seconds\\n'% (toc_ - tic_))\n",
        "\n"
      ],
      "execution_count": 334,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-334-99aa483da756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m        \u001b[0;31m# LOOP OVER MINI-BATCHES.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatchnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m           \u001b[0minput_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m           \u001b[0mtarget_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'batchnumber' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "viTHjZcxXNBG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Save stuff to model"
      ]
    },
    {
      "metadata": {
        "id": "7OwDb7hzXLiP",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 1
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "outputId": "de06bf6a-2246-48c2-d180-746a68e4ef4c",
        "executionInfo": {
          "status": "error",
          "timestamp": 1517758571126,
          "user_tz": -60,
          "elapsed": 975,
          "user": {
            "displayName": "Javier Antoran",
            "photoUrl": "//lh3.googleusercontent.com/-33SpdZ5pNBY/AAAAAAAAAAI/AAAAAAAAM2k/UTfVAawQnOQ/s50-c-k-no/photo.jpg",
            "userId": "116655854548442559250"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "    model.word_embedding_weights = word_embedding_weights\n",
        "    model.embed_to_hid_weights = embed_to_hid_weights\n",
        "    model.hid_to_output_weights = hid_to_output_weights\n",
        "    model.hid_bias = hid_bias\n",
        "    model.output_bias = output_bias\n",
        "    model.vocab = vocab"
      ],
      "execution_count": 335,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-335-14eae792d84c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embedding_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embedding_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_to_hid_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_to_hid_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid_to_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhid_to_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhid_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_bias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "bKPGJ7S3XVJy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Testing Time"
      ]
    },
    {
      "metadata": {
        "id": "8ouAAq-aIRdb",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "output_extras": [
            {
              "item_id": 4
            },
            {
              "item_id": 5
            }
          ],
          "base_uri": "https://localhost:8080/",
          "height": 334
        },
        "outputId": "c7a37854-926d-4290-e4bb-011151f0bb48",
        "executionInfo": {
          "status": "error",
          "timestamp": 1517755867609,
          "user_tz": -60,
          "elapsed": 3482,
          "user": {
            "displayName": "Javier Antoran",
            "photoUrl": "//lh3.googleusercontent.com/-33SpdZ5pNBY/AAAAAAAAAAI/AAAAAAAAM2k/UTfVAawQnOQ/s50-c-k-no/photo.jpg",
            "userId": "116655854548442559250"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "    # EVALUATE ON VALIDATION SET.\n",
        "    print('Running validation ...')\n",
        "\n",
        "    [embedding_layer_state, hidden_layer_state, output_layer_state] = fprop(valid_input, word_embedding_weights, embed_to_hid_weights, hid_to_output_weights, hid_bias, output_bias)\n",
        "    datasetsize = valid_input.shape[1]\n",
        "    expanded_valid_target = expansion_matrix[:, valid_target - 1]\n",
        "    CE = -np.sum(np.sum(np.multiply(expanded_valid_target, np.log(output_layer_state + tiny)))) / datasetsize\n",
        "    print('Final Validation CE %.3f\\n' % CE)\n",
        "\n",
        "    # EVALUATE ON TEST SET.\n",
        "    print('Running test ...')\n",
        " \n",
        "    [embedding_layer_state, hidden_layer_state, output_layer_state] = fprop(test_input, word_embedding_weights, embed_to_hid_weights, hid_to_output_weights, hid_bias, output_bias)\n",
        "    datasetsize = test_input.shape[1]\n",
        "    expanded_test_target = expansion_matrix[:, test_target-1]\n",
        "    CE = -np.sum(np.sum(np.multiply(expanded_test_target, np.log(output_layer_state + tiny)))) / datasetsize\n",
        "    print('Final Test CE %.3f' % CE)\n",
        "\n",
        "  "
      ],
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running validation ...\n",
            "(46568,)\n",
            "Final Validation CE 2.905\n",
            "\n",
            "Running test ...\n",
            "Final Test CE 2.902\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-327-c6cb7937c539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Final Test CE %.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mCE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embedding_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embedding_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed_to_hid_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_to_hid_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhid_to_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhid_to_output_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "_VQOQ4FmSYJu",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}